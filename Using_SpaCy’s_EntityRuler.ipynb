{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Key Concepts in this Notebook¶\n",
        "\n",
        "- pipe\n",
        "- factory\n",
        "- EntityRuler\n",
        "- PhraseMatcher\n",
        "- Matcher"
      ],
      "metadata": {
        "id": "ogMR0zo24OKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Einführung in Spacys EntityRuler¶\n",
        "\n",
        "Die Python-Bibliothek spaCy bietet verschiedene Methoden zur Durchführung von regelbasiertem NER. Eine solche Methode ist der EntityRuler.\n",
        "\n",
        "Der EntityRuler ist eine SpaCy-Fabrik, die es ermöglicht, eine Reihe von Mustern mit entsprechenden Beschriftungen zu erstellen. Eine Factory in spaCy ist eine Reihe von in spaCy vorinstallierten Klassen und Funktionen, die festgelegte Aufgaben ausführen. Im Fall des EntityRuler ermöglicht die vorliegende Factory dem Benutzer, einen EntityRuler zu erstellen, ihm eine Reihe von Anweisungen zu geben und diese Anweisungen dann zum Suchen und Beschriften von Entitäten zu verwenden.\n",
        "\n",
        "Sobald der Benutzer den EntityRuler erstellt und ihm eine Reihe von Anweisungen gegeben hat, kann er ihn als neue Pipe zur spaCy-Pipeline hinzufügen. Ich habe in früheren Notizbüchern kurz über Pfeifen gesprochen, aber vielleicht ist es sinnvoll, hier ausführlicher darauf einzugehen.\n",
        "\n",
        "Ein Rohr ist ein Bestandteil einer Rohrleitung. Der Zweck einer Pipeline besteht darin, Eingabedaten zu erfassen, bestimmte Operationen an diesen Eingabedaten auszuführen und diese Operationen dann entweder als neue Daten oder als extrahierte Metadaten auszugeben. Ein Rohr ist ein einzelner Bestandteil einer Rohrleitung. Im Fall von spaCy gibt es einige verschiedene Pipes, die unterschiedliche Aufgaben erfüllen. Der Tokenizer tokenisiert den Text in einzelne Token; Der Parser analysiert den Text, und der NER identifiziert Entitäten und beschriftet sie entsprechend. Alle diese Daten werden im Doc-Objekt gespeichert, wie wir in Notebook 01_01 dieser Serie gesehen haben.\n",
        "\n",
        "Es ist wichtig, sich daran zu erinnern, dass Pipelines sequentiell sind. Dies bedeutet, dass Komponenten früher in einer Pipeline Einfluss darauf haben, was spätere Komponenten erhalten. Manchmal ist diese Reihenfolge wichtig, was bedeutet, dass spätere Rohre von früheren Rohren abhängen. In anderen Fällen ist diese Reihenfolge nicht unbedingt erforderlich, was bedeutet, dass spätere Pipes ohne frühere Pipes funktionieren können. Es ist wichtig, dies im Hinterkopf zu behalten, wenn Sie benutzerdefinierte spaCy-Modelle (oder jede andere Pipeline) erstellen.\n",
        "\n",
        "In diesem Notizbuch werden wir uns den EntityRuler als Komponente der Pipeline eines SpaCy-Modells genauer ansehen. Standardmäßige spaCy-Modelle sind mit einem NER-Modell vorinstalliert; Sie werden jedoch nicht mit einem EntityRuler geliefert. Um einen EntityRuler in ein spaCy-Modell zu integrieren, muss er als neue Pipe erstellt, mit Anweisungen versehen und dann dem Modell hinzugefügt werden. Sobald dies abgeschlossen ist, kann der Benutzer das neue Modell mit dem EntityRuler auf der Festplatte speichern.\n",
        "\n",
        "Die vollständige Dokumentation von spaCy EntityRuler finden Sie hier: https://spacy.io/api/entityruler.\n",
        "\n",
        "Dieses Notizbuch fasst diese Dokumentation für Laien zusammen und bietet einige Beispiele dafür in Aktion."
      ],
      "metadata": {
        "id": "aqoy7ueo4qlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demonstration von EntityRuler in Aktion¶\n",
        "\n",
        "Im folgenden Code werden wir eine neue Pipe in das handelsübliche kleine englische Modell von spaCy einführen. Der Zweck dieses EntityRulers besteht darin, kleine Dörfer in Polen korrekt zu identifizieren."
      ],
      "metadata": {
        "id": "ERsUfFVV40PK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the requsite library\n",
        "import spacy\n",
        "\n",
        "#Buold upon the Spacy Small Model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#Sample text\n",
        "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
        "\n",
        "#Create the Doc object\n",
        "doc = nlp(text)\n",
        "\n",
        "#extract entities\n",
        "for ent in doc.ents:\n",
        "  print(ent.text,ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFfUPyAy4XIw",
        "outputId": "71ded9ce-a67f-4e83-e921-09283ce5ad84"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treblinka GPE\n",
            "Poland GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abhängig von der von Ihnen verwendeten Modellversion können einige Ergebnisse variieren.\n",
        "\n",
        "Die Ausgabe des obigen Codes zeigt die kleinen Modelle von spaCy zur Identifizierung von Treblinka, einem kleinen Dorf in Polen. Wie aus dem Beispieltext hervorgeht, war es im Zweiten Weltkrieg auch ein Vernichtungslager. Im ersten Satz markierte das spaCy-Modell Treblinka als LOC (Standort) und im zweiten Satz wurde es völlig übersehen. Beides ist entweder ungenau oder falsch. Ich hätte ORG für den zweiten Satz akzeptiert, da das Modell von spaCy nicht weiß, wie ein Vernichtungslager zu klassifizieren ist, aber diese Ergebnisse zeigen, dass das Modell nicht in der Lage ist, Daten zu verallgemeinern. Der Grund? Es gibt ein paar, aber ich vermute, dass dem Model das Wort Treblinka nie begegnet ist.\n",
        "\n",
        "Dies ist ein häufiges Problem im NLP für bestimmte Domänen. Oftmals scheitern die Domänen, in denen wir Modelle einsetzen möchten, von der Stange, weil sie nicht auf domänenspezifische Texte trainiert wurden. Wir können dieses Problem jedoch entweder über den EntityRuler von spaCy oder durch das Training eines neuen Modells lösen. Wie wir in den nächsten Notizbüchern sehen werden, können wir mit dem EntityRuler von spaCy beides problemlos erreichen.\n",
        "\n",
        "Lassen Sie uns das Problem zunächst beheben, indem wir dem Modell Anweisungen zur korrekten Identifizierung von Treblinka geben. Der Einfachheit halber verwenden wir das GPE-Label von spaCy. In einem späteren Notizbuch werden wir ein Modell vermitteln, um Treblinka im letztgenannten Kontext korrekt als Konzentrationslager zu identifizieren."
      ],
      "metadata": {
        "id": "F5r-Na-h6Gqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the requisite library\n",
        "import spacy\n",
        "\n",
        "#Build upon the spaCy Small Model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#Sample text\n",
        "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
        "\n",
        "#Create the EntityRuler\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "#List of Entities and Patterns\n",
        "patterns = [\n",
        "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
        "            ]\n",
        "\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "#extract entities\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S_LtACp4Xf7",
        "outputId": "4e58fa9e-47b8-44c0-c90a-5168b93761d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treblinka GPE\n",
            "Poland GPE\n",
            "Treblinka GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wenn Sie den obigen Code ausgeführt haben und festgestellt haben, dass Sie die gleiche Ausgabe hatten, dann haben Sie alles richtig gemacht. Diese Methode ist fehlgeschlagen. Warum? Die Antwort liegt im Konzept der Pipelines. Wir haben den EntityRuler erstellt und zur Pipeline des spaCy-Modells hinzugefügt, aber standardmäßig fügt spaCy am Ende der Pipeline eine neue Pipe hinzu. Um die Pipeline zu visualisieren, verwenden wir analysierte_pipes() von spaCy."
      ],
      "metadata": {
        "id": "13js8Ql-6ZEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.analyze_pipes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T53ULlPe4XpO",
        "outputId": "3ac6e186-7fe4-4e96-d356-f7fcaa5c1d22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
              "   'requires': [],\n",
              "   'scores': [],\n",
              "   'retokenizes': False},\n",
              "  'tagger': {'assigns': ['token.tag'],\n",
              "   'requires': [],\n",
              "   'scores': ['tag_acc'],\n",
              "   'retokenizes': False},\n",
              "  'parser': {'assigns': ['token.dep',\n",
              "    'token.head',\n",
              "    'token.is_sent_start',\n",
              "    'doc.sents'],\n",
              "   'requires': [],\n",
              "   'scores': ['dep_uas',\n",
              "    'dep_las',\n",
              "    'dep_las_per_type',\n",
              "    'sents_p',\n",
              "    'sents_r',\n",
              "    'sents_f'],\n",
              "   'retokenizes': False},\n",
              "  'attribute_ruler': {'assigns': [],\n",
              "   'requires': [],\n",
              "   'scores': [],\n",
              "   'retokenizes': False},\n",
              "  'lemmatizer': {'assigns': ['token.lemma'],\n",
              "   'requires': [],\n",
              "   'scores': ['lemma_acc'],\n",
              "   'retokenizes': False},\n",
              "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
              "   'requires': [],\n",
              "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
              "   'retokenizes': False},\n",
              "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
              "   'requires': [],\n",
              "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
              "   'retokenizes': False}},\n",
              " 'problems': {'tok2vec': [],\n",
              "  'tagger': [],\n",
              "  'parser': [],\n",
              "  'attribute_ruler': [],\n",
              "  'lemmatizer': [],\n",
              "  'ner': [],\n",
              "  'entity_ruler': []},\n",
              " 'attrs': {'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
              "  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
              "  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
              "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
              "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
              "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
              "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
              "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
              "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
              "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []}}}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dies kann zunächst etwas schwierig zu lesen sein, aber es zeigt uns die Reihenfolge, in der unsere Pipes eingerichtet sind, und ein paar andere wichtige Informationen zu jeder Pipe. Wenn wir „ner“ finden, bemerken wir, dass „entity_ruler“ dahinter steht.\n",
        "\n",
        "Damit unser EntityRuler Vorrang hat, müssen wir ihn nach der „ner“-Pipe zuweisen, wie das folgende Beispiel in dieser Zeile zeigt:\n",
        "\n",
        "__Ruler__ = __nlp.add_pipe(“entity_ruler”, after=“ner”)__"
      ],
      "metadata": {
        "id": "ZIxEhZaA6rWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build upon the spaCy Small Model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#Sample text\n",
        "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
        "\n",
        "#Create the EntityRuler\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", after=\"ner\")\n",
        "\n",
        "#List of Entities and Patterns\n",
        "patterns = [\n",
        "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
        "            ]\n",
        "\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "#extract entities\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAhQsZVW4Xr7",
        "outputId": "b358b20f-073f-4694-8081-ef60d8474fda"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treblinka GPE\n",
            "Poland GPE\n",
            "Treblinka GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beachten Sie jetzt, dass unser EntityRuler vor der „ner“-Pipe funktioniert und daher Entitäten vorab findet und sie beschriftet, bevor die NER sie erreicht. Da es früher in der Pipeline kommt, haben seine Metadaten Vorrang vor der späteren „ner“-Pipe."
      ],
      "metadata": {
        "id": "wLk-njTs64X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Einführung komplexer Regeln und Varianz in den EntityRuler (Erweitert)¶\n",
        "\n",
        "\n",
        "In manchen Fällen können Beschriftungen eine festgelegte Art von Varianz aufweisen, die einem oder mehreren bestimmten Mustern folgt. Ein solches Beispiel (in der spaCy-Dokumentation enthalten) sind Telefonnummern. In den Vereinigten Staaten gibt es verschiedene Formen von Telefonnummern. Die standardmäßige formale Methode ist (xxx)-xxx-xxxx, es ist jedoch nicht ungewöhnlich, xxx-xxx-xxxx oder xxxxxxxxxx zu sehen. Wenn der Inhaber der Telefonnummer dieselbe Nummer an jemanden außerhalb der USA weitergibt, dann +1(xxx)-xxx-xxxx.\n",
        "\n",
        "Wenn Sie in einer US-amerikanischen Domäne arbeiten, können Sie RegEx-Formeln an den Mustervergleicher übergeben, um alle diese Instanzen zu erfassen.\n",
        "\n",
        "Der spaCy EntityRuler ermöglicht es dem Benutzer auch, eine Vielzahl komplexer Regeln und Varianzen einzuführen (unter anderem über RegEx), indem er die Regeln an das Muster übergibt. Es gibt viele Argumente, die man den Mustern anführen kann. Eine vollständige Liste finden Sie unter: https://spacy.io/usage/rule-based-matching. Um herauszufinden, wie diese funktionieren, empfehle ich die Verwendung der spaCy Matcher-Demo: https://explosion.ai/demos/matcher.\n",
        "\n",
        "Im folgenden Beispiel arbeiten wir mit einem Beispiel aus der spaCy-Dokumentation, in dem wir eine Telefonnummer aus einem Text extrahieren. Dieselbe Aufgabe kann auch über RegEx erledigt werden."
      ],
      "metadata": {
        "id": "uS0S0OuZ6_Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the requisite library\n",
        "import spacy\n",
        "\n",
        "#Sample text\n",
        "text = \"This is a sample number (555) 555-5555.\"\n",
        "\n",
        "#Build upon the spaCy Small Model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "#Create the Ruler and Add it\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
        "patterns = [\n",
        "                {\"label\": \"PHONE_NUMBER\", \"pattern\": [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddd\"},\n",
        "                {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]}\n",
        "            ]\n",
        "#add patterns to ruler\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "\n",
        "\n",
        "#create the doc\n",
        "doc = nlp(text)\n",
        "\n",
        "#extract entities\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU0S4IZm7GHA",
        "outputId": "98d1ce4d-c6eb-4d74-f25a-dad034d055da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(555) 555-5555 PHONE_NUMBER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8_JR8X0I7GEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G5o5F4KN7GBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use the spaCy Matcher"
      ],
      "metadata": {
        "id": "VDx3nl247f9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Example"
      ],
      "metadata": {
        "id": "k4UHj5HM7mn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher"
      ],
      "metadata": {
        "id": "9F90pzlH7F_S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"LIKE_EMAIL\": True}]\n",
        "matcher.add(\"EMAIL_ADDRESS\", [pattern])\n",
        "doc = nlp(\"This is an email address: wmattingly@aol.com\")\n",
        "matches = matcher(doc)"
      ],
      "metadata": {
        "id": "ffwOPOaD7F77"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as4v_ULb7F3D",
        "outputId": "129b86e0-203d-4765-f379-04111fd5293e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(16571425990740197027, 6, 7)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp.vocab[matches[0][0]].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSkDnoOr4Xuf",
        "outputId": "e1585407-228b-4955-9bf4-076fc301603f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMAIL_ADDRESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Von Matcher übernommene Attribute\n",
        "\n",
        "- ORTH - The exact verbatim of a token (str)\n",
        "- TEXT - The exact verbatim of a token (str)\n",
        "- LOWER - The lowercase form of the token text (str)\n",
        "- LENGTH - The length of the token text (int)\n",
        "- IS_ALPHA\n",
        "- IS_ASCII\n",
        "- IS_DIGIT\n",
        "- IS_LOWER\n",
        "- IS_UPPER\n",
        "- IS_TITLE\n",
        "- IS_PUNCT\n",
        "- IS_SPACE\n",
        "- IS_STOP\n",
        "- IS_SENT_START\n",
        "- LIKE_NUM\n",
        "- LIKE_URL\n",
        "- LIKE_EMAIL\n",
        "- SPACY\n",
        "- POS\n",
        "- TAG\n",
        "- MORPH\n",
        "- DEP\n",
        "- LEMMA\n",
        "- SHAPE\n",
        "- ENT_TYPE\n",
        "- - Custom extension attributes (Dict[str, Any])\n",
        "- OP"
      ],
      "metadata": {
        "id": "ne9ZcEuI806c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZBEE_V349Q-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Grabbing all Proper Nouns"
      ],
      "metadata": {
        "id": "roLgyuIn9uCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"POS\": \"PROPN\"}]\n",
        "matcher.add(\"PROPER_NOUNS\", [pattern])\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "print (len(matches))\n",
        "for match in matches[:10]:\n",
        "    print (match, doc[match[1]:match[2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "568JdtL29Q7b",
        "outputId": "17058323-1fe7-45c7-edf8-4689133b0cc7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
        "matcher.add(\"PROPER_NOUNS\", [pattern])\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "print (len(matches))\n",
        "for match in matches[:10]:\n",
        "    print (match, doc[match[1]:match[2]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXECyp2_9Q4F",
        "outputId": "b04a06a7-527d-4704-9c89-9730b5773cc6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
        "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "print (len(matches))\n",
        "for match in matches[:10]:\n",
        "    print (match, doc[match[1]:match[2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfwI-aTv9Qsi",
        "outputId": "7c9793d6-d253-4555-ccbc-0695aede2e93"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}, {\"POS\": \"VERB\"}]\n",
        "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "matches.sort(key = lambda x: x[1])\n",
        "print (len(matches))\n",
        "for match in matches[:10]:\n",
        "    print (match, doc[match[1]:match[2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ltlLMOT-bWo",
        "outputId": "7dbc7caf-1dd3-4f47-c6f4-d5c89b1fc699"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open (\"data/alice.json\", \"r\") as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "QJHrI7U2-bd8",
        "outputId": "a6f27a01-110a-4709-cf6a-89df0562f2e3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-783473e17d1a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"data/alice.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/alice.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "speak_lemmas = [\"think\", \"say\"]\n",
        "text = data[0][2][0].replace( \"`\", \"'\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern1 = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}, {'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
        "pattern2 = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
        "pattern3 = [{\"POS\": \"PROPN\", \"OP\": \"+\"},{\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
        "matcher.add(\"PROPER_NOUNS\", [pattern1, pattern2, pattern3], greedy='LONGEST')\n",
        "for text in data[0][2]:\n",
        "    text = text.replace(\"`\", \"'\")\n",
        "    doc = nlp(text)\n",
        "    matches = matcher(doc)\n",
        "    matches.sort(key = lambda x: x[1])\n",
        "    print (len(matches))\n",
        "    for match in matches[:10]:\n",
        "        print (match, doc[match[1]:match[2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "atlJ61bh-bhK",
        "outputId": "c3fd8af1-4a21-4a61-8f66-cfdb20b16e0b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-cdef4df49833>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspeak_lemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"think\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"say\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"`\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpattern1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ORTH'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"'\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'IS_ALPHA'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OP\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"+\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'IS_PUNCT'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OP\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"*\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'ORTH'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"'\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"POS\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"VERB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LEMMA\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"IN\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspeak_lemmas\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"POS\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"PROPN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OP\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"+\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'ORTH'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"'\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'IS_ALPHA'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OP\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"+\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'IS_PUNCT'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OP\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"*\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'ORTH'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"'\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpattern2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ORTH'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"'\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'IS_ALPHA'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OP\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"+\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'IS_PUNCT'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OP\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"*\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'ORTH'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"'\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"POS\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"VERB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LEMMA\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"IN\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspeak_lemmas\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"POS\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"PROPN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OP\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"+\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Use RegEx in Python\n"
      ],
      "metadata": {
        "id": "cyVOteE_-u-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "IximYeUq-y5R"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r\"((\\d){1,2} (January|February|March|April|May|June|July|August|September|October|November|December))\"\n",
        "\n",
        "text = \"This is a date 2 February. Another date would be 14 August.\"\n",
        "matches = re.findall(pattern, text)\n",
        "print (matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufdJcBif-y2b",
        "outputId": "e51de46a-be07-4b11-d9e5-db0f18ec7b72"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('2 February', '2', 'February'), ('14 August', '4', 'August')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a date February 2. Another date would be 14 August.\"\n",
        "matches = re.findall(pattern, text)\n",
        "print (matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSp7dxR2-yy-",
        "outputId": "e5c9df19-2429-4e81-cb91-e88acadb8162"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('14 August', '4', 'August')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r\"(((\\d){1,2}( (January|February|March|April|May|June|July|August|September|October|November|December)))|(((January|February|March|April|May|June|July|August|September|October|November|December) )(\\d){1,2}))\"\n",
        "\n",
        "text = \"This is a date February 2. Another date would be 14 August.\"\n",
        "matches = re.findall(pattern, text)\n",
        "print (matches)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEW4PT-s-yvi",
        "outputId": "37c2ca4f-ecf9-4595-f813-3929fde20bb3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('February 2', '', '', '', '', 'February 2', 'February ', 'February', '2'), ('14 August', '14 August', '4', ' August', 'August', '', '', '', '')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a date February 2. Another date would be 14 August.\"\n",
        "iter_matches = re.finditer(pattern, text)\n",
        "print (iter_matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX4nI51x_EK_",
        "outputId": "6da168be-3906-45fd-af88-ff668d614ed1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<callable_iterator object at 0x7f45483171c0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a date February 2. Another date would be 14 August.\"\n",
        "iter_matches = re.finditer(pattern, text)\n",
        "print (iter_matches)\n",
        "for hit in iter_matches:\n",
        "    print (hit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt-F-5K7_EHB",
        "outputId": "3159cc2e-c33d-4784-9d47-a089beed52be"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<callable_iterator object at 0x7f4548316200>\n",
            "<re.Match object; span=(15, 25), match='February 2'>\n",
            "<re.Match object; span=(49, 58), match='14 August'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a date February 2. Another date would be 14 August.\"\n",
        "iter_matches = re.finditer(pattern, text)\n",
        "for hit in iter_matches:\n",
        "    start = hit.start()\n",
        "    end = hit.end()\n",
        "    print (text[start:end])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMpYXYXF_Duq",
        "outputId": "7ff9722e-3fcc-40b4-f5e5-83a16e9f44dd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "February 2\n",
            "14 August\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Use RegEx in spaCy"
      ],
      "metadata": {
        "id": "1J--EfWm_S5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the requisite library\n",
        "import spacy\n",
        "\n",
        "#Sample text\n",
        "text = \"This is a sample number 555-5555.\"\n",
        "\n",
        "#Build upon the spaCy Small Model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "#Create the Ruler and Add it\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
        "patterns = [\n",
        "                {\"label\": \"PHONE_NUMBER\", \"pattern\": [{\"SHAPE\": \"ddd\"},\n",
        "                {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]}\n",
        "            ]\n",
        "#add patterns to ruler\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "#create the doc\n",
        "doc = nlp(text)\n",
        "\n",
        "#extract entities\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVblyfMG_Uv3",
        "outputId": "9045c1dd-64f8-45e5-e214-5773c80b7886"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "555-5555 PHONE_NUMBER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r\"((\\d){3}-(\\d){4})\"\n",
        "text = \"This is a sample number 555-5555.\"\n",
        "matches = re.findall(pattern, text)\n",
        "print (matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HwMqbXt_eWl",
        "outputId": "de5b7eac-9809-4bae-9b79-f517dc25e904"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('555-5555', '5', '5')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the requisite library\n",
        "import spacy\n",
        "\n",
        "#Sample text\n",
        "text = \"This is a sample number (555) 555-5555.\"\n",
        "\n",
        "#Build upon the spaCy Small Model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "#Create the Ruler and Add it\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
        "patterns = [\n",
        "                {\n",
        "                    \"label\": \"PHONE_NUMBER\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"((\\d){3}-(\\d){4})\"}}\n",
        "                                                        ]\n",
        "                }\n",
        "            ]\n",
        "#add patterns to ruler\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "\n",
        "#create the doc\n",
        "doc = nlp(text)\n",
        "\n",
        "#extract entities\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "0V6wEtPv_ilc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the requisite library\n",
        "import spacy\n",
        "\n",
        "#Sample text\n",
        "text = \"This is a sample number (555) 555-5555.\"\n",
        "\n",
        "#Build upon the spaCy Small Model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "#Create the Ruler and Add it\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
        "patterns = [\n",
        "                {\n",
        "                    \"label\": \"PHONE_NUMBER\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"((\\d){3}-(\\d){4})\"}}\n",
        "                                                        ]\n",
        "                }\n",
        "            ]\n",
        "#add patterns to ruler\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "\n",
        "#create the doc\n",
        "doc = nlp(text)\n",
        "\n",
        "#extract entities\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "EL-I2XI4_jJy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with Multi-Word Token Entities and RegEx in spaCy 3\n"
      ],
      "metadata": {
        "id": "qoQdwQGm_s-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
        "\n",
        "pattern = r\"Paul [A-Z]\\w+\"\n",
        "\n",
        "matches = re.finditer(pattern, text)\n",
        "\n",
        "for match in matches:\n",
        "    print (match)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gmohi9S_uhC",
        "outputId": "e7da6d90-e164-47e8-c651-eca64ea67b65"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 11), match='Paul Newman'>\n",
            "<re.Match object; span=(39, 53), match='Paul Hollywood'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "from spacy.tokens import Span"
      ],
      "metadata": {
        "id": "dFaavz7J_3w6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
        "pattern = r\"Paul [A-Z]\\w+\""
      ],
      "metadata": {
        "id": "XLr-b7fc_34G"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "jZPSDbF1_37t"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_ents = list(doc.ents)"
      ],
      "metadata": {
        "id": "HNaQ5p1u_3_j"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mwt_ents = []\n",
        "for match in re.finditer(pattern, doc.text):\n",
        "    start, end = match.span()\n",
        "    span = doc.char_span(start, end)\n",
        "    if span is not None:\n",
        "        mwt_ents.append((span.start, span.end, span.text))"
      ],
      "metadata": {
        "id": "rn-Y_UP8ABpy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in mwt_ents:\n",
        "    start, end, name = ent\n",
        "    per_ent = Span(doc, start, end, label=\"PERSON\")\n",
        "    original_ents.append(per_ent)"
      ],
      "metadata": {
        "id": "jraR1_MKAB1z"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc.ents = original_ents"
      ],
      "metadata": {
        "id": "jNFFoJiZAEpy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-PRo5sBAEud",
        "outputId": "07a8661f-118e-4a5f-da3f-37733c2ff60f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Newman PERSON\n",
            "Paul Hollywood PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Geben Sie größeren Spannen Vorrang"
      ],
      "metadata": {
        "id": "ZzRUPT5zAML9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "\n",
        "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host.\"\n",
        "pattern = r\"Hollywood\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "6mkgxQGxAEyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mwt_ents = []\n",
        "original_ents = list(doc.ents)\n",
        "for match in re.finditer(pattern, doc.text):\n",
        "    print (match)\n",
        "    start, end = match.span()\n",
        "    span = doc.char_span(start, end)\n",
        "    if span is not None:\n",
        "        mwt_ents.append((span.start, span.end, span.text))\n",
        "for ent in mwt_ents:\n",
        "    start, end, name = ent\n",
        "    per_ent = Span(doc, start, end, label=\"CINEMA\")\n",
        "    original_ents.append(per_ent)\n",
        "\n",
        "doc.ents = original_ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "ymHtyYULAE35",
        "outputId": "d7e04e43-79d6-424b-f67d-3dde162ade54"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 11), match='Paul Newman'>\n",
            "<re.Match object; span=(39, 53), match='Paul Hollywood'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-425d356ded45>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moriginal_ents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_ent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_ents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E1010] Unable to set entity information for token 0 which is included in more than one span in entities, blocked, missing or outside."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.util import filter_spans\n",
        "filtered = filter_spans(original_ents)\n",
        "doc.ents = filtered\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuPNIjaUAE7z",
        "outputId": "6f4b33e0-3e9a-4947-f1fa-516d4fe4125a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Newman PERSON\n",
            "Paul Hollywood PERSON\n"
          ]
        }
      ]
    }
  ]
}